# -*- coding: utf-8 -*-
"""Inventory ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mi662VkEo6Q9-SDZK6tXpZVP3wrNjv4u
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib as matplotlib
import seaborn as sns
import plotly.express as px
import numpy as np
import pandas as pd
# %matplotlib inline


sns.set_style("darkgrid")
matplotlib.rcParams['font.size']=14
matplotlib.rcParams['figure.figsize']=(10,5)

from google.colab import drive
drive.mount('/content/drive')
file_path="/content/drive/MyDrive/Colab Notebooks/Datasets/inventory_data.csv"
df=pd.read_csv(file_path,encoding='latin1')
df

df.shape

df.info()

df.describe()

print(df.isnull().sum())

print((df['price'] <= 0).sum())

print(df['transaction_type'].value_counts())

sku_counts = df['sku_id'].value_counts().reset_index()
sku_counts.columns = ['sku_id', 'transaction_count']


fig = px.bar(sku_counts.head(20), x='sku_id', y='transaction_count',
             title='Top 20 SKUs by Number of Transactions')
fig.show()

tx_counts = df['transaction_type'].value_counts().reset_index()
tx_counts.columns = ['transaction_type', 'count']

fig = px.pie(tx_counts, names='transaction_type', values='count',
             title='Transaction Type Distribution')
fig.show()

fig = px.histogram(df, x='current_stock', nbins=50, title='Current Stock Distribution')
fig.show()

fig = px.scatter(df.sample(5000), x='qty_in', y='qty_out', color='transaction_type',
                 title='Qty In vs Qty Out (Sample of 5k)')
fig.show()

df['date'] = pd.to_datetime(df['timestamp']).dt.date
daily_sales = df.groupby('date')['qty_out'].sum().reset_index()

fig = px.line(daily_sales, x='date', y='qty_out', title='Daily Total Sales Over Time')
fig.show()

loc_sales = df.groupby('location')['qty_out'].sum().reset_index()

fig = px.bar(loc_sales, x='location', y='qty_out', title='Total Sales per Location')
fig.show()

fig = px.histogram(df, x='expiry_date', nbins=50, title='Expiry Date Distribution')
fig.show()

df['timestamp'] = pd.to_datetime(df['timestamp'])

df['year_month'] = df['timestamp'].dt.to_period('M')

monthly_sales = df.groupby(['sku_id', 'year_month'])['qty_out'].sum().reset_index()
monthly_sales.rename(columns={'qty_out':'monthly_sales'}, inplace=True)

monthly_sales.head()

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder
from sklearn.ensemble import RandomForestRegressor
import joblib


def train_and_save_pipeline(csv_path, save_path="demand_model.joblib"):
    df = pd.read_csv(csv_path)


    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['year'] = df['timestamp'].dt.year
    df['month'] = df['timestamp'].dt.month
    df['day'] = df['timestamp'].dt.day
    df['day_of_week'] = df['timestamp'].dt.dayofweek

    target_col = 'qty_out'
    input_cols = [c for c in df.columns if c not in ['transaction_id', target_col, 'timestamp']]

    X = df[input_cols].copy()
    y = df[target_col].copy()

    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()


    X[numeric_cols] = X[numeric_cols].astype(float)


    imputer = SimpleImputer(strategy='mean')

    X[numeric_cols] = imputer.fit_transform(X[numeric_cols])

    scaler = MinMaxScaler()
    X[numeric_cols] = pd.DataFrame(
        scaler.fit_transform(X[numeric_cols]),
        columns=numeric_cols,
        index=X.index
    )


    encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
    X[categorical_cols] = pd.DataFrame(
        encoder.fit_transform(X[categorical_cols]),
        columns=categorical_cols,
        index=X.index
    )


    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        max_features='sqrt',
        n_jobs=-1,
        random_state=42
    )
    model.fit(X, y)


    pipeline = {
        'model': model,
        'imputer': imputer,
        'scaler': scaler,
        'encoder': encoder,
        'input_cols': input_cols,
        'numeric_cols': numeric_cols,
        'categorical_cols': categorical_cols
    }
    joblib.dump(pipeline, save_path)
    print(f"Pipeline saved as {save_path}")
    return pipeline


def preprocess(df, pipeline):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['year'] = df['timestamp'].dt.year
    df['month'] = df['timestamp'].dt.month
    df['day'] = df['timestamp'].dt.day
    df['day_of_week'] = df['timestamp'].dt.dayofweek

    X = df[pipeline['input_cols']].copy()


    X[pipeline['numeric_cols']] = X[pipeline['numeric_cols']].astype(float)
    X[pipeline['numeric_cols']] = pd.DataFrame(
        pipeline['imputer'].transform(X[pipeline['numeric_cols']]),
        columns=pipeline['numeric_cols'],
        index=X.index
    )
    X[pipeline['numeric_cols']] = pd.DataFrame(
        pipeline['scaler'].transform(X[pipeline['numeric_cols']]),
        columns=pipeline['numeric_cols'],
        index=X.index
    )


    X[pipeline['categorical_cols']] = pd.DataFrame(
        pipeline['encoder'].transform(X[pipeline['categorical_cols']]),
        columns=pipeline['categorical_cols'],
        index=X.index
    )

    return X

pipeline = train_and_save_pipeline("/content/drive/MyDrive/Colab Notebooks/Datasets/inventory_data.csv")

new_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Datasets/inventory_data.csv")
X_new = preprocess(new_df, pipeline)
predictions = pipeline['model'].predict(X_new)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(new_df['qty_out'], predictions)
rmse = np.sqrt(mean_squared_error(new_df['qty_out'], predictions))
r2 = r2_score(new_df['qty_out'], predictions)

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")
print(f"R²: {r2}")

import gradio as gr
import pandas as pd
import numpy as np
import joblib
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load pipeline
pipeline = joblib.load("demand_model.joblib")

def preprocess_for_gradio(df, pipeline):
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['year'] = df['timestamp'].dt.year
    df['month'] = df['timestamp'].dt.month
    df['day'] = df['timestamp'].dt.day
    df['day_of_week'] = df['timestamp'].dt.dayofweek

    X = df[pipeline['input_cols']].copy()
    X[pipeline['numeric_cols']] = X[pipeline['numeric_cols']].astype(float)
    X[pipeline['numeric_cols']] = pd.DataFrame(
        pipeline['imputer'].transform(X[pipeline['numeric_cols']]),
        columns=pipeline['numeric_cols'],
        index=X.index
    )
    X[pipeline['numeric_cols']] = pd.DataFrame(
        pipeline['scaler'].transform(X[pipeline['numeric_cols']]),
        columns=pipeline['numeric_cols'],
        index=X.index
    )
    X[pipeline['categorical_cols']] = pd.DataFrame(
        pipeline['encoder'].transform(X[pipeline['categorical_cols']]),
        columns=pipeline['categorical_cols'],
        index=X.index
    )
    return X

def create_advanced_visualizations(df, predictions):
    """Create comprehensive visualization dashboard"""
    df['predicted_qty_out'] = predictions

    # 1. Top 10 SKUs by Predicted Demand
    top_skus = df.groupby('sku_id')['predicted_qty_out'].sum().reset_index()
    top_skus = top_skus.sort_values('predicted_qty_out', ascending=False).head(10)
    fig1 = px.bar(top_skus, x='sku_id', y='predicted_qty_out',
                  title="Top 10 SKUs by Predicted Demand",
                  color='predicted_qty_out',
                  color_continuous_scale='Blues',
                  labels={'predicted_qty_out': 'Predicted Quantity'})
    fig1.update_layout(showlegend=False, height=400)

    # 2. Demand by Location
    location_demand = df.groupby('location')['predicted_qty_out'].sum().reset_index()
    fig2 = px.pie(location_demand, values='predicted_qty_out', names='location',
                  title='Demand Distribution by Location',
                  color_discrete_sequence=px.colors.sequential.RdBu)
    fig2.update_layout(height=400)

    # 3. Monthly Demand Trend
    monthly_trend = df.groupby(['year', 'month'])['predicted_qty_out'].sum().reset_index()
    monthly_trend['period'] = monthly_trend['year'].astype(str) + '-' + monthly_trend['month'].astype(str).str.zfill(2)
    fig3 = px.line(monthly_trend, x='period', y='predicted_qty_out',
                   title='Monthly Demand Forecast Trend',
                   markers=True,
                   labels={'predicted_qty_out': 'Total Predicted Quantity', 'period': 'Time Period'})
    fig3.update_traces(line_color='#1f77b4', line_width=3)
    fig3.update_layout(height=400)

    # 4. Transaction Type Distribution
    trans_dist = df.groupby('transaction_type')['predicted_qty_out'].sum().reset_index()
    fig4 = px.bar(trans_dist, x='transaction_type', y='predicted_qty_out',
                  title='Demand by Transaction Type',
                  color='transaction_type',
                  color_discrete_sequence=px.colors.qualitative.Set2)
    fig4.update_layout(showlegend=False, height=400)

    return fig1, fig2, fig3, fig4

def batch_predict(csv_file, show_top_n):
    try:
        df = pd.read_csv(csv_file.name)
        X = preprocess_for_gradio(df, pipeline)
        predictions = pipeline['model'].predict(X)
        df['predicted_qty_out'] = predictions

        # Create insights dataframe
        insight_df = df[['sku_id', 'transaction_type', 'location', 'year', 'month', 'predicted_qty_out']].copy()
        insight_df['predicted_qty_out'] = insight_df['predicted_qty_out'].round(2)

        # Compute detailed metrics if actuals exist
        metrics_summary = ""
        comparison_fig = None

        if 'qty_out' in df.columns:
            mae = mean_absolute_error(df['qty_out'], predictions)
            rmse = np.sqrt(mean_squared_error(df['qty_out'], predictions))
            r2 = r2_score(df['qty_out'], predictions)
            mape = np.mean(np.abs((df['qty_out'] - predictions) / df['qty_out'])) * 100

            metrics_summary = f"""
**Model Performance Metrics**
- Mean Absolute Error (MAE): {mae:.2f}
- Root Mean Squared Error (RMSE): {rmse:.2f}
- R² Score: {r2:.4f}
- Mean Absolute Percentage Error (MAPE): {mape:.2f}%
            """

            # Actual vs Predicted comparison
            comparison_df = df[['qty_out', 'predicted_qty_out']].head(100)
            comparison_fig = go.Figure()
            comparison_fig.add_trace(go.Scatter(y=comparison_df['qty_out'], mode='lines+markers',
                                                name='Actual', line=dict(color='green', width=2)))
            comparison_fig.add_trace(go.Scatter(y=comparison_df['predicted_qty_out'], mode='lines+markers',
                                                name='Predicted', line=dict(color='orange', width=2)))
            comparison_fig.update_layout(title='Actual vs Predicted Demand (First 100 Records)',
                                        xaxis_title='Record Index',
                                        yaxis_title='Quantity',
                                        height=400)
        else:
            metrics_summary = "No actual values found in dataset. Upload data with 'qty_out' column for performance metrics."

        # Generate advanced visualizations
        fig1, fig2, fig3, fig4 = create_advanced_visualizations(df, predictions)

        # Summary statistics
        total_demand = df['predicted_qty_out'].sum()
        avg_demand = df['predicted_qty_out'].mean()
        max_demand_sku = df.loc[df['predicted_qty_out'].idxmax(), 'sku_id']

        summary_text = f"""
### Forecast Summary
- **Total Predicted Demand**: {total_demand:,.2f} units
- **Average Demand per SKU**: {avg_demand:.2f} units
- **Highest Demand SKU**: {max_demand_sku}
- **Total Records Processed**: {len(df):,}
        """

        # Top N SKUs detail
        top_n_df = df.groupby('sku_id').agg({
            'predicted_qty_out': ['sum', 'mean', 'count']
        }).reset_index()
        top_n_df.columns = ['sku_id', 'total_demand', 'avg_demand', 'transaction_count']
        top_n_df = top_n_df.sort_values('total_demand', ascending=False).head(show_top_n)
        top_n_df = top_n_df.round(2)

        return (insight_df, metrics_summary, summary_text, top_n_df,
                fig1, fig2, fig3, fig4, comparison_fig)

    except Exception as e:
        error_msg = f"Error processing file: {str(e)}"
        return pd.DataFrame(), error_msg, "", pd.DataFrame(), None, None, None, None, None

def single_predict(sku_id, transaction_type, location, year, month, day):
    try:
        df = pd.DataFrame([{
            "sku_id": sku_id,
            "transaction_type": transaction_type,
            "location": location,
            "year": int(year),
            "month": int(month),
            "day": int(day),
            "day_of_week": pd.Timestamp(f"{int(year)}-{int(month)}-{int(day)}").dayofweek
        }])

        X = preprocess_for_gradio(df, pipeline)
        pred = pipeline['model'].predict(X)[0]

        result_df = pd.DataFrame([{
            'SKU ID': sku_id,
            'Transaction Type': transaction_type,
            'Location': location,
            'Date': f"{int(year)}-{int(month)}-{int(day)}",
            'Predicted Demand': round(pred, 2)
        }])

        # Create gauge chart for single prediction
        fig = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=pred,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Predicted Demand", 'font': {'size': 24}},
            delta={'reference': pred * 0.8},
            gauge={
                'axis': {'range': [None, pred * 1.5]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, pred * 0.5], 'color': "lightgray"},
                    {'range': [pred * 0.5, pred], 'color': "gray"}],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': pred * 1.2}}))
        fig.update_layout(height=400)

        interpretation = f"""
### Prediction Interpretation
- **Predicted Quantity**: {pred:.2f} units
- **Confidence Level**: Based on historical patterns
- **Recommendation**: {"High priority restocking" if pred > 100 else "Normal inventory levels"}
        """

        return result_df, fig, interpretation

    except Exception as e:
        error_msg = f" Error: {str(e)}"
        return pd.DataFrame(), None, error_msg

# Custom CSS for better styling
custom_css = """
.gradio-container {
    font-family: 'Arial', sans-serif;
}
.tab-nav button {
    font-size: 16px;
    font-weight: bold;
}
h1 {
    background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
}
"""

# Main Gradio Interface
with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # Advanced Inventory Demand Forecasting System
    ### Powered by Machine Learning | Real-time Predictions & Analytics
    """)

    with gr.Tabs():
        # Batch Prediction Tab
        with gr.TabItem("Batch Prediction & Analytics"):
            gr.Markdown("""
            ### Upload your inventory CSV file for comprehensive demand forecasting
            **Required columns**: `sku_id`, `transaction_type`, `location`, `timestamp`
            **Optional**: `qty_out` (for performance evaluation)
            """)

            with gr.Row():
                with gr.Column(scale=2):
                    csv_input = gr.File(label="Upload Inventory CSV", file_types=[".csv"])
                with gr.Column(scale=1):
                    top_n_slider = gr.Slider(minimum=5, maximum=50, value=10, step=5,
                                            label="Top N SKUs to Display")

            process_btn = gr.Button("Process & Generate Forecast", variant="primary", size="lg")

            gr.Markdown("---")

            with gr.Row():
                with gr.Column():
                    summary_output = gr.Markdown(label="Summary")
                with gr.Column():
                    metrics_output = gr.Markdown(label="Performance Metrics")

            gr.Markdown("### Top SKUs Analysis")
            top_n_output = gr.Dataframe(label="Detailed SKU Rankings")

            gr.Markdown("### Visualization Dashboard")
            with gr.Row():
                plot1 = gr.Plot(label="Top SKUs")
                plot2 = gr.Plot(label="Location Distribution")

            with gr.Row():
                plot3 = gr.Plot(label="Temporal Trends")
                plot4 = gr.Plot(label="Transaction Analysis")

            comparison_plot = gr.Plot(label="Actual vs Predicted Comparison")

            gr.Markdown("### Full Prediction Results")
            output_df = gr.Dataframe(label="Detailed Predictions", interactive=False)

            process_btn.click(
                batch_predict,
                inputs=[csv_input, top_n_slider],
                outputs=[output_df, metrics_output, summary_output, top_n_output,
                        plot1, plot2, plot3, plot4, comparison_plot]
            )

        # Help & Info Tab
        with gr.TabItem("Help & Information"):
            gr.Markdown("""
            ## User Guide

            ### Batch Prediction
            1. **Prepare your CSV** with the following columns:
               - `sku_id`: Unique product identifier
               - `transaction_type`: sale, return, or transfer
               - `location`: Warehouse or store location
               - `timestamp`: Date-time of transaction
               - `qty_out` (optional): Actual quantity for model evaluation

            2. **Upload the file** and adjust the "Top N SKUs" slider
            3. **Click "Process & Generate Forecast"** to get comprehensive analytics

            ### Understanding the Metrics
            - **MAE**: Average prediction error (lower is better)
            - **RMSE**: Penalizes large errors more (lower is better)
            - **R² Score**: Model fit quality (closer to 1 is better)
            - **MAPE**: Error as percentage of actual values

            ### Visualizations Explained
            - **Top SKUs Chart**: Identifies highest demand products
            - **Location Distribution**: Shows demand across locations
            - **Temporal Trends**: Reveals seasonal patterns
            - **Transaction Analysis**: Breaks down by transaction type

            ### Tips for Best Results
            - Ensure data quality and completeness
            - Include diverse time periods for better trends
            - Use actual values when available for validation
            - Monitor predictions against actual performance

            ---
            **Version**: 2.0 | **Model**: Advanced ML Pipeline | **Last Updated**: 2025
            """)

demo.launch(share=False, server_name="0.0.0.0")
